#!/usr/bin/env python3
"""
Demo script to showcase the Real-Time Web Server Log Analysis system
"""

import time
import threading
import subprocess
import sys
import os
import json

# Add parent directory to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from log_generator.generate_logs import LogGenerator
from hadoop.mapreduce_jobs import LogAnalyzer
from analytics.real_time_analyzer import RealTimeAnalyzer

class DemoRunner:
    def __init__(self):
        self.generator = LogGenerator()
        self.analyzer = LogAnalyzer()
        self.running = False
        
    def generate_sample_data(self, num_logs=100000):  # Default to 100K logs for big data feel
        """Generate sample log data for demonstration"""
        print(f"üîÑ Generating {num_logs:,} sample log entries for big data analysis...")
        print("This simulates a busy e-commerce website with realistic traffic patterns...")
        
        sample_logs = []
        batch_size = 10000  # Process in batches to show progress
        
        # Ensure data directory exists
        os.makedirs('data', exist_ok=True)
        sample_file = 'data/demo_logs.json'
        
        # Generate logs in batches and write incrementally
        with open(sample_file, 'w') as f:
            for batch_start in range(0, num_logs, batch_size):
                batch_end = min(batch_start + batch_size, num_logs)
                batch_logs = []
                
                print(f"üìä Processing batch {batch_start//batch_size + 1}/{(num_logs-1)//batch_size + 1} ({batch_start:,} - {batch_end:,})")
                
                for i in range(batch_start, batch_end):
                    log_entry = self.generator.generate_log_entry()
                    
                    # Parse log entry
                    import re
                    from config.config import LOG_FORMAT
                    
                    pattern = LOG_FORMAT['apache_common']
                    match = re.match(pattern, log_entry)
                    
                    if match:
                        log_data = {
                            'ip': match.group(1),
                            'timestamp': match.group(2),
                            'method': match.group(3),
                            'url': match.group(4),
                            'protocol': match.group(5),
                            'status_code': int(match.group(6)),
                            'response_size': match.group(7),
                            'raw_log': log_entry
                        }
                        batch_logs.append(log_data)
                        sample_logs.append(log_data)
                
                # Write batch to file
                for log in batch_logs:
                    f.write(json.dumps(log) + '\n')
                
                # Show progress
                progress = (batch_end / num_logs) * 100
                print(f"   ‚úÖ Progress: {progress:.1f}% ({len(sample_logs):,} logs generated)")
        
        # Calculate file size
        file_size = os.path.getsize(sample_file)
        file_size_mb = file_size / (1024 * 1024)
        
        print(f"\nüéâ BIG DATA GENERATION COMPLETE!")
        print(f"üìÅ File: {sample_file}")
        print(f"üìä Total Records: {len(sample_logs):,}")
        print(f"üíæ File Size: {file_size_mb:.2f} MB")
        print(f"üîç This represents {len(sample_logs)//1000} minutes of high-traffic website logs")
        
        return sample_file
    
    def run_mapreduce_demo(self, sample_file):
        """Demonstrate MapReduce analysis"""
        print("\n" + "="*70)
        print("üîß HADOOP MAPREDUCE BIG DATA ANALYSIS")
        print("="*70)
        
        # Show file statistics
        if os.path.exists(sample_file):
            file_size = os.path.getsize(sample_file)
            file_size_mb = file_size / (1024 * 1024)
            
            # Count lines (records)
            with open(sample_file, 'r') as f:
                record_count = sum(1 for _ in f)
            
            print(f"üìÅ Dataset: {sample_file}")
            print(f"üìä Records: {record_count:,}")
            print(f"üíæ Size: {file_size_mb:.2f} MB")
            print(f"üîç Processing {record_count:,} log entries across distributed MapReduce jobs...")
            print()
        
        print("üöÄ Starting Distributed MapReduce Jobs...")
        print()
        
        print("1Ô∏è‚É£ ERROR PATTERN ANALYSIS")
        self.analyzer.analyze_errors(sample_file)
        
        print("\n2Ô∏è‚É£ TRAFFIC PEAK ANALYSIS")
        self.analyzer.analyze_peak_usage(sample_file)
        
        print("\n3Ô∏è‚É£ SECURITY ANOMALY DETECTION")
        # Dynamic threshold based on data size
        threshold = max(5, record_count // 10000) if 'record_count' in locals() else 5
        self.analyzer.analyze_ip_anomalies(sample_file, threshold=threshold)
        
        print("\n4Ô∏è‚É£ CONTENT POPULARITY ANALYSIS")
        self.analyzer.analyze_url_popularity(sample_file)
        
        print("\n" + "="*70)
        print("‚úÖ BIG DATA MAPREDUCE ANALYSIS COMPLETE")
        print("="*70)
    
    def simulate_real_time_logs(self, duration=60):
        """Simulate real-time log generation"""
        print(f"\nüîÑ Simulating real-time logs for {duration} seconds...")
        
        end_time = time.time() + duration
        log_count = 0
        
        while time.time() < end_time and self.running:
            # Generate normal traffic
            for _ in range(5):  # 5 logs per second
                log_entry = self.generator.generate_log_entry()
                print(f"[LOG {log_count}] {log_entry}")
                log_count += 1
                time.sleep(0.2)
            
            # Occasionally generate error bursts
            if log_count % 50 == 0:  # Every 50 logs
                print("\nüö® Generating error burst...")
                for _ in range(3):
                    error_log = self.generator.generate_log_entry()
                    # Force it to be an error by modifying
                    error_log = error_log.replace(' 200 ', ' 500 ')
                    print(f"[ERROR] {error_log}")
                    log_count += 1
                print()
        
        print(f"‚úÖ Generated {log_count} real-time log entries")
    
    def run_interactive_demo(self):
        """Run interactive demonstration"""
        print("üöÄ REAL-TIME WEB SERVER LOG ANALYSIS - INTERACTIVE DEMO")
        print("="*70)
        
        while True:
            print("\nChoose a demo option:")
            print("1. Generate sample data and run MapReduce analysis")
            print("   üìä Small dataset (10K logs) - Quick demo")
            print("   üìä Medium dataset (100K logs) - Realistic big data")
            print("   üìä Large dataset (500K logs) - Enterprise scale")
            print("2. Simulate real-time log generation")
            print("3. Show system architecture")
            print("4. Run full end-to-end demo (50K logs)")
            print("5. Exit")
            
            choice = input("\nEnter your choice (1-5): ").strip()
            
            if choice == '1':
                print("\nChoose dataset size:")
                print("1. Small (10K logs) - Quick demo (~2MB)")
                print("2. Medium (100K logs) - Big data feel (~20MB)")
                print("3. Large (500K logs) - Enterprise scale (~100MB)")
                
                size_choice = input("Enter size choice (1-3): ").strip()
                
                if size_choice == '1':
                    sample_file = self.generate_sample_data(10000)
                elif size_choice == '2':
                    sample_file = self.generate_sample_data(100000)
                elif size_choice == '3':
                    sample_file = self.generate_sample_data(500000)
                else:
                    print("Invalid choice, using medium dataset...")
                    sample_file = self.generate_sample_data(100000)
                
                self.run_mapreduce_demo(sample_file)
                
            elif choice == '2':
                duration = input("Enter duration in seconds (default 30): ").strip()
                duration = int(duration) if duration.isdigit() else 30
                self.running = True
                try:
                    self.simulate_real_time_logs(duration)
                except KeyboardInterrupt:
                    print("\n‚èπÔ∏è  Simulation stopped by user")
                finally:
                    self.running = False
                    
            elif choice == '3':
                self.show_architecture()
                
            elif choice == '4':
                self.run_full_demo()
                
            elif choice == '5':
                print("üëã Goodbye!")
                break
                
            else:
                print("‚ùå Invalid choice. Please try again.")
    
    def show_architecture(self):
        """Display system architecture"""
        print("\n" + "="*70)
        print("üèóÔ∏è  SYSTEM ARCHITECTURE")
        print("="*70)
        
        architecture = """
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   Web Server    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Log Generator  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Kafka Producer ‚îÇ
        ‚îÇ   (Apache/Nginx)‚îÇ    ‚îÇ  (Simulated)    ‚îÇ    ‚îÇ                 ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                                ‚îÇ
                                                                ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ  Real-Time      ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ Kafka Consumer  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ  Kafka Cluster  ‚îÇ
        ‚îÇ  Analytics      ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ  (Topic: logs)  ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ                                                ‚îÇ
                ‚ñº                                                ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   Alerts &      ‚îÇ    ‚îÇ  Hadoop HDFS    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   MapReduce     ‚îÇ
        ‚îÇ   Dashboard     ‚îÇ    ‚îÇ   (Storage)     ‚îÇ    ‚îÇ  (Batch Jobs)   ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        
        Data Flow:
        1. üåê Web servers generate access logs
        2. üì° Kafka streams logs in real-time
        3. üíæ HDFS stores logs for historical analysis
        4. ‚ö° Real-time analytics detect anomalies
        5. üîß MapReduce runs batch analysis jobs
        6. üìä Dashboard shows insights and alerts
        """
        
        print(architecture)
        
        print("\nKey Components:")
        print("‚Ä¢ Kafka: High-throughput message streaming")
        print("‚Ä¢ Hadoop HDFS: Distributed file storage")
        print("‚Ä¢ MapReduce: Parallel batch processing")
        print("‚Ä¢ Real-time Analytics: Immediate anomaly detection")
        print("‚Ä¢ Dashboard: Live monitoring and alerts")
    
    def run_full_demo(self):
        """Run complete end-to-end demonstration"""
        print("\n" + "="*70)
        print("üéØ FULL END-TO-END DEMONSTRATION")
        print("="*70)
        
        print("\nüìã Demo Steps:")
        print("1. Generate sample historical data")
        print("2. Run MapReduce batch analysis")
        print("3. Simulate real-time log streaming")
        print("4. Show real-time analytics")
        
        input("\nPress Enter to start the demo...")
        
        # Step 1: Generate data
        print("\nüîÑ Step 1: Generating sample data...")
        sample_file = self.generate_sample_data(50000)  # 50K logs for full demo
        
        # Step 2: Batch analysis
        print("\nüîÑ Step 2: Running batch analysis...")
        self.run_mapreduce_demo(sample_file)
        
        # Step 3: Real-time simulation
        print("\nüîÑ Step 3: Starting real-time simulation...")
        print("(This will run for 30 seconds - press Ctrl+C to stop early)")
        
        self.running = True
        try:
            self.simulate_real_time_logs(30)
        except KeyboardInterrupt:
            print("\n‚èπÔ∏è  Demo stopped by user")
        finally:
            self.running = False
        
        print("\n‚úÖ Full demo completed!")
        print("\nIn a production environment:")
        print("‚Ä¢ Kafka would handle millions of logs per second")
        print("‚Ä¢ HDFS would store petabytes of historical data")
        print("‚Ä¢ Real-time analytics would trigger immediate alerts")
        print("‚Ä¢ MapReduce jobs would run on large clusters")

if __name__ == "__main__":
    demo = DemoRunner()
    
    if len(sys.argv) > 1 and sys.argv[1] == '--auto':
        # Run automated demo
        demo.run_full_demo()
    else:
        # Run interactive demo
        demo.run_interactive_demo()